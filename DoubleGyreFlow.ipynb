{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoTomasetto/SHRED-ROM/blob/main/DoubleGyreFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CbWevFiEbY2"
      },
      "outputs": [],
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kwBmroMK-oj"
      },
      "source": [
        "##**Double gyre flow**\n",
        "\n",
        "The double gyre flow is a time-dependent model for two counter-rotating vortices (gyres) in a rectangular domain. When time is introduced via a periodic perturbation, the central dividing line between the two gyres oscillates left and right, creating a time-varying velocity field that can lead to chaotic particle trajectories. The velocity field $\\mathbf{v} = [u, v]^T$ in the domain $[0, L_x] \\times [0, L_y]$ and in the time interval $[0, T]$ is given by\n",
        "\n",
        "\\\n",
        "$$\n",
        "\\begin{align}\n",
        "u(x, y, t) &= -\\pi I \\sin\\left( \\pi f(x, t) \\right) \\cos\\left( \\pi y \\right)\n",
        "\\\\\n",
        "v(x, y, t) &= \\pi I \\cos\\left( \\pi f(x, t) \\right) \\sin\\left( \\pi y \\right) \\frac{\\partial f}{\\partial x}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\\\n",
        "where $I$ is the intensity parameter, $f(x, t) = \\epsilon \\sin(\\omega t) x^2 + (1 - 2\\epsilon \\sin(\\omega t)) x $, $\\epsilon$ and $\\omega$ are the perturbation amplitude and the frequency of the oscillation, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUxnTm66AA5h"
      },
      "outputs": [],
      "source": [
        "# DEFINE THE SYSTEM SOLVER\n",
        "\n",
        "def double_gyre_flow(amplitude, frequency, x, y, t):\n",
        "    '''\n",
        "    Solve the double gyre flow problem\n",
        "\n",
        "    Inputs\n",
        "        amplitude                   (`float`)\n",
        "        frequency                   (`float`)\n",
        "        horizontal discretization   (`np.array[float]`, shape: (ny,))\n",
        "        vertical discretization     (`np.array[float]`, shape: (nx,))\n",
        "        time vector                 (`np.array[float]`, shape: (ntimes,))\n",
        "\n",
        "    Output\n",
        "        horizontal velocity matrix  (`np.array[float]`, shape: (ntimes, nx * ny)\n",
        "        vertical velocity matrix    (`np.array[float]`, shape: (ntimes, nx * ny)\n",
        "    '''\n",
        "\n",
        "    xgrid, ygrid = np.meshgrid(x, y)         # spatial grid\n",
        "\n",
        "    u = np.zeros((len(t), len(x), len(y)))   # horizontal velocity\n",
        "    v = np.zeros((len(t), len(x), len(y)))   # vertical velocity\n",
        "\n",
        "    intensity = 0.1   # intensity parameter\n",
        "\n",
        "    f = lambda x,t: amplitude * np.sin(frequency * t) * x**2 + x - 2 * amplitude * np.sin(frequency * t) * x\n",
        "    flux = lambda x,y,t: intensity * np.sin(np.pi * f(x,t)) * np.sin(np.pi * y)\n",
        "\n",
        "    # compute solution\n",
        "    for i in range(len(t)):\n",
        "      u[i] = (-np.pi * intensity * np.sin(np.pi * f(xgrid, t[i])) * np.cos(np.pi * ygrid)).T\n",
        "      v[i] = (np.pi * intensity * np.cos(np.pi * f(xgrid, t[i])) * np.sin(np.pi * ygrid) * (2 * amplitude * np.sin(frequency * t[i]) * xgrid + 1.0 - 2 * amplitude * np.sin(frequency * t[i]))).T\n",
        "\n",
        "    return u, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bpJjlxEAA5h"
      },
      "outputs": [],
      "source": [
        "# SOLVE THE SYSTEM FOR A FIXED TRANSPORT TERM\n",
        "\n",
        "amplitude = 0.25    # amplitude\n",
        "frequency = 5.0     # frequency\n",
        "\n",
        "# spatial discretization\n",
        "nx = 50\n",
        "ny = 25\n",
        "Lx = 2.0\n",
        "Ly = 1.0\n",
        "x = np.linspace(0, Lx, nx)\n",
        "y = np.linspace(0, Ly, ny)\n",
        "nstate = len(x) * len(y)\n",
        "\n",
        "# temporal discretization\n",
        "dt = 0.1\n",
        "T = 10.0\n",
        "t = np.arange(0, T + dt, dt)\n",
        "ntimes = len(t)\n",
        "\n",
        "u, v = double_gyre_flow(amplitude, frequency, x, y, t)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUTION VISUALIZATION\n",
        "\n",
        "from ipywidgets import interact, FloatSlider\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def vorticity(u, v):\n",
        "    dx = Lx / nx\n",
        "    dy = Ly / ny\n",
        "    du_dy = np.gradient(u, dy, axis = 1)\n",
        "    dv_dx = np.gradient(v, dx, axis = 0)\n",
        "    return dv_dx - du_dy\n",
        "\n",
        "def plot_solution(time):\n",
        "\n",
        "    which_time = (np.abs(t - time)).argmin()\n",
        "    offset = 0.1\n",
        "\n",
        "    plt.figure(figsize = (10,5))\n",
        "    plt.contourf(x, y, vorticity(u[which_time], v[which_time]).T, cmap = 'seismic', levels = 100)\n",
        "    plt.streamplot(x, y, u[which_time].T, v[which_time].T, color='black', linewidth = 1, density = 1)\n",
        "    plt.axis('off')\n",
        "    plt.axis([0 - offset, Lx + offset, 0 - offset, Ly + offset])\n",
        "    plt.title(f'Solution at time t = {round(time, 3)}')\n",
        "    plt.grid(True)\n",
        "    plt.gca().add_patch(patches.Rectangle((0, 0), Lx, Ly, linewidth = 5, edgecolor = 'black', facecolor = 'none'))\n",
        "\n",
        "\n",
        "interact(plot_solution, time = FloatSlider(value = t[0], min = t[0], max = t[-1], step = (t[1]-t[0]), description='time', layout={'width': '400px', 'height': '50px'}));"
      ],
      "metadata": {
        "id": "FpktNKcfFDIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xyJgYLj7LFr"
      },
      "outputs": [],
      "source": [
        "# DATA GENERATION\n",
        "\n",
        "amplitude_range = np.array([0.0, 0.5])\n",
        "frequency_range = np.array([0.5, 2*np.pi])\n",
        "\n",
        "# spatial discretization\n",
        "nx = 50\n",
        "ny = 25\n",
        "Lx = 2.0\n",
        "Ly = 1.0\n",
        "x = np.linspace(0, Lx, nx)\n",
        "y = np.linspace(0, Ly, ny)\n",
        "nstate = len(x) * len(y)\n",
        "\n",
        "# temporal discretization\n",
        "dt = 0.1\n",
        "T = 10.0\n",
        "t = np.arange(0, T + dt, dt)\n",
        "ntimes = len(t)\n",
        "\n",
        "# training data generation\n",
        "ntrajectories = 100\n",
        "U = np.zeros((ntrajectories, ntimes, nx, ny))\n",
        "V = np.zeros((ntrajectories, ntimes, nx, ny))\n",
        "\n",
        "for i in range(ntrajectories):\n",
        "  amplitude = (amplitude_range[1] - amplitude_range[0]) * np.random.rand() + amplitude_range[0]\n",
        "  frequency = (frequency_range[1] - frequency_range[0]) * np.random.rand() + frequency_range[0]\n",
        "  U[i], V[i] = double_gyre_flow(amplitude, frequency, x, y, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ea0qe0w9J4C"
      },
      "outputs": [],
      "source": [
        "# DATA VISUALIZATION\n",
        "\n",
        "from ipywidgets import interact, IntSlider\n",
        "\n",
        "def plot_data(which_trajectory, which_time):\n",
        "\n",
        "    offset = 0.1\n",
        "\n",
        "    plt.figure(figsize = (10,5))\n",
        "    plt.contourf(x, y, vorticity(U[which_trajectory, which_time], V[which_trajectory, which_time]).T, cmap = 'seismic', levels = 100)\n",
        "    plt.streamplot(x, y, U[which_trajectory, which_time].T, V[which_trajectory, which_time].T, color='black', linewidth = 1, density = 1)\n",
        "    plt.axis('off')\n",
        "    plt.axis([0 - offset, Lx + offset, 0 - offset, Ly + offset])\n",
        "    plt.title(f'Trajectory {which_trajectory} at time t = {round(t[which_time], 3)}')\n",
        "    plt.grid(True)\n",
        "    plt.gca().add_patch(patches.Rectangle((0, 0), Lx, Ly, linewidth = 5, edgecolor = 'black', facecolor = 'none'))\n",
        "\n",
        "interact(plot_data, which_trajectory = IntSlider(min = 0, max = ntrajectories - 1, step = 1, description='Trajectory'), which_time = IntSlider(min = 0, max = ntimes - 1, step = 1, description='Time step'));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOdWLTImTq5v"
      },
      "outputs": [],
      "source": [
        "# TORCH CONVERSION\n",
        "\n",
        "U = torch.from_numpy(U.reshape(ntrajectories, ntimes, nstate))\n",
        "V = torch.from_numpy(V.reshape(ntrajectories, ntimes, nstate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rEn7LNMTLTh"
      },
      "outputs": [],
      "source": [
        "# TRAIN-VALIDATION SPLITTING\n",
        "\n",
        "split_ratio = 0.8\n",
        "\n",
        "ntrain = round(split_ratio * ntrajectories)\n",
        "\n",
        "idx_train = np.random.choice(ntrajectories, size = ntrain, replace = False)\n",
        "mask = np.ones(ntrajectories)\n",
        "mask[idx_train] = 0\n",
        "idx_valid_test = np.arange(0, ntrajectories)[np.where(mask!=0)[0]]\n",
        "idx_valid = idx_valid_test[::2]\n",
        "idx_test = idx_valid_test[1::2]\n",
        "\n",
        "nvalid = idx_valid.shape[0]\n",
        "ntest = idx_test.shape[0]\n",
        "\n",
        "Utrain = U[idx_train]\n",
        "Uvalid = U[idx_valid]\n",
        "Utest = U[idx_test]\n",
        "\n",
        "Vtrain = V[idx_train]\n",
        "Vvalid = V[idx_valid]\n",
        "Vtest = V[idx_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyLRM0qgBQQD"
      },
      "source": [
        "## **SHallow REcurrent Decoder networks-based Reduced Order Modeling (SHRED-ROM)**\n",
        "\n",
        "Let us assume to have three sensors in the domain measuring the horizontal velocity $u(x_s,y_s,t;\\epsilon, \\omega)$ over time. *SHRED-ROM* aims to reconstruct the temporal evolution of the entire velocity $\\mathbf{v}(x,y,t;\\epsilon, \\omega) = [u(x,y,t;\\epsilon, \\omega), v(x,y,t;\\epsilon, \\omega)]^T$ starting from the limited sensor measurements available. In general, *SHRED-ROM* combines a recurrent neural network (LSTM), which encodes the temporal history of sensor values in multiple parametric regimes, and a shallow decoder, which projects the LSTM prediction to the (possibly high-dimensional) state dimension. Note that, to enhance computational efficiency and memory usage, dimensionality reduction strategies (such as, e.g., POD) may be considered to compress the training snapshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0igaVBBtipO"
      },
      "outputs": [],
      "source": [
        "# DIMENSIONALITY REDUCTION\n",
        "\n",
        "Utrain = Utrain.reshape(-1, nstate)\n",
        "Uvalid = Uvalid.reshape(-1, nstate)\n",
        "Utest = Utest.reshape(-1, nstate)\n",
        "\n",
        "U1_POD, S1_POD, V1_POD = np.linalg.svd(Utrain, full_matrices = False)\n",
        "POD_modes = 4\n",
        "\n",
        "Utrain_proj = Utrain @ V1_POD[:POD_modes, :].T\n",
        "Utrain_proj = Utrain_proj.reshape(ntrain, ntimes, POD_modes)\n",
        "Utrain = Utrain.reshape(ntrain, ntimes, nstate)\n",
        "\n",
        "Uvalid_proj = Uvalid @ V1_POD[:POD_modes, :].T\n",
        "Uvalid_proj = Uvalid_proj.reshape(nvalid, ntimes, POD_modes)\n",
        "Uvalid = Uvalid.reshape(nvalid, ntimes, nstate)\n",
        "\n",
        "Utest_proj = Utest @ V1_POD[:POD_modes, :].T\n",
        "Utest_proj = Utest_proj.reshape(ntest, ntimes, POD_modes)\n",
        "Utest = Utest.reshape(ntest, ntimes, nstate)\n",
        "\n",
        "print(f\"Residual energy is equal to  {np.sum(S1_POD[POD_modes:]**2) / np.sum(S1_POD**2)}\\n\")\n",
        "plt.plot(S1_POD[:100], color = 'magenta', linewidth = 2)\n",
        "plt.axvline(POD_modes, color = 'teal', linestyle = '--', linewidth = 2)\n",
        "plt.title(\"Singular Values Decay\");"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DIMENSIONALITY REDUCTION\n",
        "\n",
        "Vtrain = Vtrain.reshape(-1, nstate)\n",
        "Vvalid = Vvalid.reshape(-1, nstate)\n",
        "Vtest = Vtest.reshape(-1, nstate)\n",
        "\n",
        "U2_POD, S2_POD, V2_POD = np.linalg.svd(Vtrain, full_matrices = False)\n",
        "POD_modes = 4\n",
        "\n",
        "Vtrain_proj = Vtrain @ V2_POD[:POD_modes, :].T\n",
        "Vtrain_proj = Vtrain_proj.reshape(ntrain, ntimes, POD_modes)\n",
        "Vtrain = Vtrain.reshape(ntrain, ntimes, nstate)\n",
        "\n",
        "Vvalid_proj = Vvalid @ V2_POD[:POD_modes, :].T\n",
        "Vvalid_proj = Vvalid_proj.reshape(nvalid, ntimes, POD_modes)\n",
        "Vvalid = Vvalid.reshape(nvalid, ntimes, nstate)\n",
        "\n",
        "Vtest_proj = Vtest @ V2_POD[:POD_modes, :].T\n",
        "Vtest_proj = Vtest_proj.reshape(ntest, ntimes, POD_modes)\n",
        "Vtest = Vtest.reshape(ntest, ntimes, nstate)\n",
        "\n",
        "print(f\"Residual energy is equal to  {np.sum(S2_POD[POD_modes:]**2) / np.sum(S2_POD**2)}\\n\")\n",
        "plt.plot(S2_POD[:100], color = 'magenta', linewidth = 2)\n",
        "plt.axvline(POD_modes, color = 'teal', linestyle = '--', linewidth = 2)\n",
        "plt.title(\"Singular Values Decay\");"
      ],
      "metadata": {
        "id": "lKPC9bk1f7kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WopbyANFqRa"
      },
      "outputs": [],
      "source": [
        "# DEFINE FUNCTION TO PROCESS DATA\n",
        "\n",
        "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Define a dataset of input-output time-series\n",
        "\n",
        "    Inputs\n",
        "        sequence of input measurements    (`torch.Tensor[float]`, shape: (ntrajectories, ntimes, ninput))\n",
        "        state measurements                (`torch.Tensor[float]`, shape: (ntrajectories, ntimes, noutput))\n",
        "\n",
        "    Output\n",
        "        Torch dataset                     (`torch.utils.data.Dataset`)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.len = X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "def Padding(data, lag):\n",
        "    '''\n",
        "    Extract time-series of lenght equal to lag from longer time series in data\n",
        "\n",
        "    Inputs\n",
        "        time-series data          (`torch.Tensor[float]`, shape: (ntrajectories, ntimes, ninput))\n",
        "        lag parameter             (`int`)\n",
        "\n",
        "    Input\n",
        "        lagged time-series data   (`torch.Tensor[float]`, shape: (ntrajectories * ntimes, lag, ninput))\n",
        "    '''\n",
        "\n",
        "    data_out = torch.zeros(data.shape[0] * data.shape[1], lag, data.shape[2])\n",
        "\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(1, data.shape[1] + 1):\n",
        "            if j < lag:\n",
        "                data_out[i * data.shape[1] + j - 1, -j:] = data[i, :j]\n",
        "            else:\n",
        "                data_out[i * data.shape[1] + j - 1] = data[i, j - lag : j]\n",
        "\n",
        "    return data_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysMOW5rRigZv"
      },
      "outputs": [],
      "source": [
        "# DEFINE SHRED MODEL\n",
        "\n",
        "class SHRED(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size = 64, hidden_layers = 2, decoder_sizes = [350, 400], dropout = 0.0):\n",
        "        '''\n",
        "        SHRED model definition\n",
        "\n",
        "        Inputs\n",
        "        \tinput size (e.g. number of sensors)                 (`int`)\n",
        "        \toutput size (e.g. full-order variable dimension)    (`int`)\n",
        "        \tsize of LSTM hidden layers                          (`int`)\n",
        "        \tnumber of LSTM hidden layers                        (`int`)\n",
        "        \tlist of decoder layers sizes                        (`list[int]`)\n",
        "        \tdropout parameter                                   (`float`)\n",
        "        '''\n",
        "\n",
        "        super(SHRED,self).__init__()\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size = input_size,\n",
        "                                  hidden_size = hidden_size,\n",
        "                                  num_layers = hidden_layers,\n",
        "                                  batch_first=True)\n",
        "\n",
        "        self.decoder = torch.nn.ModuleList()\n",
        "        decoder_sizes.insert(0, hidden_size)\n",
        "        decoder_sizes.append(output_size)\n",
        "\n",
        "        for i in range(len(decoder_sizes)-1):\n",
        "            self.decoder.append(torch.nn.Linear(decoder_sizes[i], decoder_sizes[i+1]))\n",
        "            if i != len(decoder_sizes)-2:\n",
        "                self.decoder.append(torch.nn.Dropout(dropout))\n",
        "                self.decoder.append(torch.nn.ReLU())\n",
        "\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h_0 = torch.zeros((self.hidden_layers, x.size(0), self.hidden_size), dtype=torch.float)\n",
        "        c_0 = torch.zeros((self.hidden_layers, x.size(0), self.hidden_size), dtype=torch.float)\n",
        "        if next(self.parameters()).is_cuda:\n",
        "            h_0 = h_0.cuda()\n",
        "            c_0 = c_0.cuda()\n",
        "\n",
        "        _, (output, _) = self.lstm(x, (h_0, c_0))\n",
        "        output = output[-1].view(-1, self.hidden_size)\n",
        "\n",
        "        for layer in self.decoder:\n",
        "            output = layer(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def freeze(self):\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "\n",
        "        self.train()\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT3F5wYNFVt1"
      },
      "outputs": [],
      "source": [
        "# DEFINE TRAINING FUNCTION\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from copy import deepcopy\n",
        "from IPython.display import clear_output as clc\n",
        "\n",
        "mse = lambda datatrue, datapred: (datatrue - datapred).pow(2).sum(axis = -1).mean()  # Mean Squared Error\n",
        "error_formatter = lambda error: \".6f\" % error\n",
        "\n",
        "def fit(model, train_dataset, valid_dataset, batch_size = 64, epochs = 4000, optim = torch.optim.Adam, lr = 1e-3, loss_fun = mse, loss_output = mse, formatter = error_formatter, verbose = False, patience = 5):\n",
        "    '''\n",
        "    Neural networks training\n",
        "\n",
        "    Inputs\n",
        "    \tmodel                                 (`torch.nn.Module`)\n",
        "    \ttraining dataset                      (`torch.Tensor`)\n",
        "    \tvalidation dataset                    (`torch.Tensor`)\n",
        "    \tbatch size                            (`int`)\n",
        "    \tnumber of epochs                      (`int`)\n",
        "    \toptimizer                             (`function`)\n",
        "    \tlearning rate                         (`float`)\n",
        "      loss function                         (`function`)\n",
        "      loss value to print and return        (`function`)\n",
        "      loss formatter for printing           (`function`)\n",
        "    \tverbose parameter                     (`bool`)\n",
        "    \tpatience parameter                    (`int`)\n",
        "    '''\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
        "    optimizer = optim(model.parameters(), lr = lr)\n",
        "\n",
        "    train_error_list = []\n",
        "    valid_error_list = []\n",
        "    patience_counter = 0\n",
        "    best_params = model.state_dict()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        for k, data in enumerate(train_loader):\n",
        "            model.train()\n",
        "            def closure():\n",
        "                outputs = model(data[0])\n",
        "                optimizer.zero_grad()\n",
        "                loss = loss_fun(outputs, data[1])\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            optimizer.step(closure)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_error = loss_output(train_dataset.Y, model(train_dataset.X))\n",
        "            valid_error = loss_output(valid_dataset.Y, model(valid_dataset.X))\n",
        "            train_error_list.append(train_error)\n",
        "            valid_error_list.append(valid_error)\n",
        "\n",
        "        if verbose == True:\n",
        "            print(\"Epoch \"+ str(epoch) + \": Training loss = \" + \"%.6f\" % train_error_list[-1] + \" \\t Validation loss = \" + \"%.6f\" % valid_error_list[-1])\n",
        "            clc(wait = True)\n",
        "\n",
        "        if valid_error == torch.min(torch.tensor(valid_error_list)):\n",
        "            patience_counter = 0\n",
        "            best_params = deepcopy(model.state_dict())\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter == patience:\n",
        "            model.load_state_dict(best_params)\n",
        "            train_error = loss_output(train_dataset.Y, model(train_dataset.X))\n",
        "            valid_error = loss_output(valid_dataset.Y, model(valid_dataset.X))\n",
        "\n",
        "            if verbose == True:\n",
        "                print(\"Training done: Training loss = \" + \"%.6f\" % train_error + \" \\t Validation loss = \" + \"%.6f\" % valid_error)\n",
        "\n",
        "            return torch.tensor(train_error_list).detach().cpu().numpy(), torch.tensor(valid_error_list).detach().cpu().numpy()\n",
        "\n",
        "    model.load_state_dict(best_params)\n",
        "    train_error = loss_output(train_dataset.Y, model(train_dataset.X))\n",
        "    valid_error = loss_output(valid_dataset.Y, model(valid_dataset.X))\n",
        "\n",
        "    if verbose == True:\n",
        "      print(\"Training done: Training loss = \" + \"%.6f\" % train_error + \" \\t Validation loss = \" + \"%.6f\" % valid_error)\n",
        "\n",
        "    return torch.tensor(train_error_list).detach().cpu().numpy(), torch.tensor(valid_error_list).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oROHRQQ1Jszl"
      },
      "outputs": [],
      "source": [
        "# EXTRACT SENSORS DATA\n",
        "\n",
        "nsensors = 3\n",
        "idx_sensors = np.random.choice(nstate, size = nsensors, replace = False)\n",
        "\n",
        "sensors_data_train = Utrain[:,:,idx_sensors]\n",
        "sensors_data_valid = Uvalid[:,:,idx_sensors]\n",
        "sensors_data_test = Utest[:,:,idx_sensors]\n",
        "\n",
        "idx_sensors_x, idx_sensors_y = np.unravel_index(idx_sensors, (nx, ny))\n",
        "sensors_coordinates = np.vstack((x[idx_sensors_x], y[idx_sensors_y]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG6gkBPsTcQs"
      },
      "outputs": [],
      "source": [
        "# BUILD TRAIN, VALIDATION AND TEST DATASETS WITH PADDING\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "lag = 25\n",
        "\n",
        "train_data_in = Padding(sensors_data_train, lag).to(device)\n",
        "valid_data_in = Padding(sensors_data_valid, lag).to(device)\n",
        "test_data_in = Padding(sensors_data_test, lag).to(device)\n",
        "\n",
        "train_data_out = Padding(torch.cat((Utrain_proj, Vtrain_proj), 2), 1).squeeze(1).to(device)\n",
        "valid_data_out = Padding(torch.cat((Uvalid_proj, Vvalid_proj), 2), 1).squeeze(1).to(device)\n",
        "test_data_out = Padding(torch.cat((Utest_proj, Vtest_proj), 2), 1).squeeze(1).to(device)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
        "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
        "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h-VdIh2fUWDl",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# SHRED TRAINING\n",
        "\n",
        "shred = SHRED(nsensors, 2*POD_modes, hidden_size = 64, hidden_layers = 2, decoder_sizes = [], dropout = 0.1).to(device)\n",
        "train_errors, valid_errors = fit(shred, train_dataset, valid_dataset, batch_size = 64, epochs = 100, lr = 1e-3, verbose = True, patience = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsfRmovokf9X"
      },
      "outputs": [],
      "source": [
        "# TRAINING HISTORY VISUALIZATION\n",
        "\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.plot(train_errors, 'k', linewidth = 3, label = 'Training error')\n",
        "plt.plot(valid_errors, 'orange', linewidth = 3, label = 'Validation error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MY5pzGmcKfW",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# SHRED RECONSTRUCTION VISUALIZATION\n",
        "\n",
        "from ipywidgets import interact, IntSlider\n",
        "\n",
        "shred.freeze()\n",
        "\n",
        "UVtest_hat = shred(test_data_in).cpu()\n",
        "\n",
        "Utest_hat = UVtest_hat[:,:POD_modes] @ V1_POD[:POD_modes, :].float()\n",
        "Utest_hat = Utest_hat.reshape(ntest, ntimes, nx, ny)\n",
        "Utest = Utest.reshape(ntest, ntimes, nx, ny)\n",
        "\n",
        "Vtest_hat = UVtest_hat[:,POD_modes:] @ V2_POD[:POD_modes, :].float()\n",
        "Vtest_hat = Vtest_hat.reshape(ntest, ntimes, nx, ny)\n",
        "Vtest = Vtest.reshape(ntest, ntimes, nx, ny)\n",
        "\n",
        "def plot_shred_reconstruction(which_test_trajectory, which_time):\n",
        "\n",
        "    offset = 0.1\n",
        "\n",
        "    plt.figure(figsize = (20,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.contourf(x, y, vorticity(Utest[which_test_trajectory, which_time],  Vtest[which_test_trajectory, which_time]).T, cmap = 'seismic', levels = 100)\n",
        "    plt.streamplot(x, y, Utest[which_test_trajectory, which_time].T, Vtest[which_test_trajectory, which_time].T, color='black', linewidth = 1, density = 1)\n",
        "    plt.axis('off')\n",
        "    plt.axis([0 - offset, Lx + offset, 0 - offset, Ly + offset])\n",
        "    plt.title(f'Test case {which_test_trajectory} at time t = {round(t[which_time], 3)}')\n",
        "    plt.grid(True)\n",
        "    plt.gca().add_patch(patches.Rectangle((0, 0), Lx, Ly, linewidth = 5, edgecolor = 'black', facecolor = 'none'))\n",
        "    for k in range(nsensors):\n",
        "      plt.plot(sensors_coordinates[0, k], sensors_coordinates[1, k], 'o', mfc = 'magenta', mec = 'black', ms = 8, mew = 1.5)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.contourf(x, y, vorticity(Utest_hat[which_test_trajectory, which_time],  Vtest_hat[which_test_trajectory, which_time]).T, cmap = 'seismic', levels = 100)\n",
        "    plt.streamplot(x, y, Utest_hat[which_test_trajectory, which_time].T, Vtest_hat[which_test_trajectory, which_time].T, color='black', linewidth = 1, density = 1)\n",
        "    plt.axis('off')\n",
        "    plt.axis([0 - offset, Lx + offset, 0 - offset, Ly + offset])\n",
        "    plt.title(f'SHRED reconstruction at time t = {round(t[which_time], 3)}')\n",
        "    plt.grid(True)\n",
        "    plt.gca().add_patch(patches.Rectangle((0, 0), Lx, Ly, linewidth = 5, edgecolor = 'black', facecolor = 'none'))\n",
        "    for k in range(nsensors):\n",
        "      plt.plot(sensors_coordinates[0, k], sensors_coordinates[1, k], 'o', mfc = 'magenta', mec = 'black', ms = 8, mew = 1.5)\n",
        "\n",
        "interact(plot_shred_reconstruction, which_test_trajectory = IntSlider(value = 0, min = 0, max = ntest - 1, description='Test case'), which_time = IntSlider(min = 0, max = ntimes - 1, step = 1, description='Time step'));"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}